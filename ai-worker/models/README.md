# Placeholder for local GGUF models

Place your GGUF model files here:

## Mini Models (1-3B parameters)
- Recommended: TinyLlama, Phi-2, StableLM
- Example: `lyra-mini.gguf` or `tinyllama-1.1b.gguf`
- RAM requirement: 4-8 GB

## Big Models (7-13B+ parameters)
- Recommended: Llama-2-7B, Mistral-7B, Llama-2-13B
- Example: `mistral-7b.gguf` or `llama-2-13b.gguf`
- RAM requirement: 16-32 GB

## Where to download:
- Hugging Face: https://huggingface.co/models?library=gguf
- TheBloke's models: https://huggingface.co/TheBloke

## Note:
Models are NOT included in this repository to keep it lightweight.
The application will work in stub/cloud mode without local models.
